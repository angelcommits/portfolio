# ğŸ“š ExplicaciÃ³n ArquitectÃ³nica y Plan de IntegraciÃ³n OLAMA

## 1. ğŸ“¦ Â¿QuÃ© son los `adapters/` en DDD?

En Domain-Driven Design, los `adapters` representan **interfaces tecnolÃ³gicas concretas** para,
los "puertos" definidos en tu dominio.
ActÃºan como puentes entre la lÃ³gica pura de negocio y las tecnologÃ­as externas. Ejemplos:

- `adapters/api/`: Interfaz de entrada. Define cÃ³mo los clientes externos (como tu frontend React o MiSitioBot) interactÃºan con el sistema. Implementan controladores que renderizan respuestas en JSON, HTML, XML, etc.
- `adapters/storage/`: Interfaz de salida. AquÃ­ implementÃ¡s cÃ³mo guardar o recuperar archivos desde S3, disco, Google Cloud, etc., sin acoplarlo directamente a tu lÃ³gica de negocio.
- `adapters/notification/`: TambiÃ©n interfaz de salida. Define adaptadores para enviar emails, mensajes de texto (Twilio), WhatsApp, webhooks, etc.

**Importante**: Esto te da flexibilidad para cambiar la implementaciÃ³n sin tocar el dominio.

---

## 2. ğŸ‘¥ Â¿CÃ³mo representa `User` los distintos roles?

Tu modelo `User` puede representar:

- `Operador`: Vos mismo, interactuando a fondo con el sistema. TenÃ©s acceso total.
- `ReceptorBot`: El bot que recepciona consultas del pÃºblico. Tiene acceso restringido a un subconjunto de la base de conocimientos.

En el futuro, podrÃ­as agregar:

- `Cliente`: Personas externas a quienes les des acceso controlado al sistema.
- `Sistema`: Automatismos que actÃºan como usuarios tÃ©cnicos.

**La clave** es usar roles (`:operator`, `:bot`, `:client`) y, si hace falta, trabajar con bases de datos lÃ³gicas o fÃ­sicas separadas para evitar fugas de informaciÃ³n.

---

## 3. ğŸ§  IntegraciÃ³n de OLAMA como motor local

### ğŸ“ Estructura recomendada

```
ollama/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ code-llama (u otro modelo)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ start_ollama.sh
â”œâ”€â”€ config/
â”‚   â””â”€â”€ ollama_config.yml
```

### âš™ï¸ Flujo de integraciÃ³n

1. **ArrancÃ¡s OLAMA como servicio local**:

   ```bash
   ollama run code-llama
   ```

2. **Desde tu backend Rails**, hacÃ©s requests HTTP a localhost:

   ```http
   POST http://localhost:11434/api/generate
   Content-Type: application/json

   {
     "model": "code-llama",
     "prompt": "EscribÃ­ un test en Ruby para esta funciÃ³n..."
   }
   ```

3. **Tu backend actÃºa como middleware**: puede decidir si usa OLAMA local o una API externa (ChatGPT) segÃºn contexto, prioridad o sensibilidad.

4. **GuardÃ¡s en tu DB** tanto la pregunta como la respuesta, como parte de tu sesiÃ³n.

---

## âœ… Plan de acciÃ³n siguiente

1. Instalar OLAMA (si no estÃ¡ activo)
2. Descargar modelo como `code-llama` o `mistral`
3. Probar localmente desde terminal
4. Implementar cliente HTTP simple en Rails (`ollama_client.rb`)
5. Conectar a `GPT::InferenceService` en tu dominio
6. Probar flujo completo desde backend
